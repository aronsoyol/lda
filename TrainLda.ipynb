{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "import sys , os, re, string\n",
    "import nltk\n",
    "import jieba\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_cn = []\n",
    "with open(\"ch_stopwords.txt\") as f_cn_stopword:\n",
    "    for line in f_cn_stopword:\n",
    "        stop_words_cn.append(line.strip(\"\\n\"))\n",
    "#     print(len(stop_words_cn))\n",
    "#     for w in stop_words_cn:\n",
    "#         print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn\n",
    "# path = \"./enpod\"\n",
    "\n",
    "class Tokenizer:\n",
    "    def if_stopword(word):\n",
    "        stop_list = stopwords.words(\"english\") + \\\n",
    "            [\n",
    "                \",\", \".\", \"!\", \"?\", \";\", \":\", \"\\n\", \"\\t\",   \\\n",
    "                \"(\", \")\", \" \", \"`\", \"'\", \"a\", \"b\", \"c\",     \\\n",
    "                \"’\", \"'m\", \"'s\", \"n't\", \"'ll\", \"'re\",       \\\n",
    "                \"'ve\", \"”\", \"“\", \"'d\", \"‘\", \"’\", \"–\"\n",
    "            ] + stop_words_cn\n",
    "        if word.lower() in stop_list:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_number(s):\n",
    "        try:\n",
    "            float(s)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            import re\n",
    "            num_reg = re.compile(\"\\\\d+,*:*'*\\\\d+\")\n",
    "            if num_reg.match(s):\n",
    "                return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            import unicodedata\n",
    "            unicodedata.numeric(s)\n",
    "            return True\n",
    "        except (TypeError, ValueError):\n",
    "            pass\n",
    "    \n",
    "        return False\n",
    "    \n",
    "    def is_ascii(s):\n",
    "        try:\n",
    "            return all(ord(c) < 256 for c in s)\n",
    "        except TypeError:\n",
    "            return False\n",
    "        \n",
    "    # https://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn\n",
    "    def stem_tokens(tokens, stemmer):\n",
    "        stemmed = []\n",
    "        for item in tokens:\n",
    "            stemmed.append(stemmer.stem(item))\n",
    "        return stemmed\n",
    "\n",
    "    def tokens(text):\n",
    "        return (nltk.word_tokenize(text))\n",
    "        \n",
    "    def stems(text):\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        tokens1 = nltk.word_tokenize(text)\n",
    "        tokens = [x.lower() for x in tokens1 if not re.fullmatch('[' + string.punctuation + ']+', x) and not Tokenizer.if_stopword(x) and not Tokenizer.is_number(x)]\n",
    "        stems = Tokenizer.stem_tokens(tokens, porter_stemmer)\n",
    "        return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, *args):\n",
    "        super(Documents, self).__init__(*args)\n",
    "        \n",
    "    def get_doc_name_list(self):\n",
    "        # return self.__doc_file_name_list\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_doc_stems_list(self):\n",
    "        raise NotImplementedError()\n",
    "        # return self.__doc_stems_list\n",
    "    def get_word_dict(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新概念英語"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NceDOC(Documents):\n",
    "    __doc_stems_list = []\n",
    "    __doc_name_list = []\n",
    "    __doc_word_dict = defaultdict(lambda x: 0)\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\") as file:\n",
    "            for i, line in enumerate(file):\n",
    "                \n",
    "                stem_list = Tokenizer.stems(line.strip())\n",
    "                word_list = Tokenizer.tokens(line.strip())\n",
    "\n",
    "                self.__doc_stems_list.append(stem_list)\n",
    "                self.__doc_name_list.append(\"Doc%02d\"%(i))\n",
    "                self.__doc_word_dict = CountVectorizer()\n",
    "                \n",
    "\n",
    "    def get_doc_name_list(self):\n",
    "        return self.__doc_name_list\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "    def get_doc_stems_list(self):\n",
    "        return self.__doc_stems_list\n",
    "\n",
    "    def get_word_dict(self):\n",
    "        return self.__doc_word_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnglishPod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpDOC(Documents):\n",
    "    '''\n",
    "    English Pod \n",
    "    '''\n",
    "    # self.__path = \"\"\n",
    "    __doc_stems_list = []\n",
    "    __doc_file_name_list = []\n",
    "\n",
    "    def __init__(self, path):\n",
    "        # super(Documents, self).__init__(*args))\n",
    "        self.__doc_stems_list = []\n",
    "        self.__path = path\n",
    "        for file_name in os.listdir(self.__path):\n",
    "            word_list = []\n",
    "            if not file_name.endswith(\"lrc\"):\n",
    "                continue\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            print(file_name, end=': ')\n",
    "            with open(file_path, \"r\") as file:\n",
    "                for i, line in enumerate(file):\n",
    "                    if i ==0 :\n",
    "                        continue\n",
    "                    stem_list = Tokenizer.stems(line.strip())\n",
    "                    # stem_tokens(tokens, porter_stemmer)\n",
    "                    for stem in stem_list:\n",
    "                        # if not if_stopword(word):\n",
    "                        print(stem, end=\", \")\n",
    "                        word_list.append(stem)\n",
    "                        \n",
    "                print(\"\")\n",
    "            self.__doc_stems_list.append(word_list)\n",
    "            self.__doc_file_name_list.append(file_name)\n",
    "\n",
    "    def get_doc_name_list(self):\n",
    "        return self.__doc_file_name_list\n",
    "\n",
    "    def get_doc_stems_list(self):\n",
    "        return self.__doc_stems_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba.suggest_freq('云计算', True)\n",
    "# jieba.suggest_freq('区块链', True)\n",
    "# jieba.suggest_freq(\"物联网\", True)\n",
    "# jieba.suggest_freq(\"大数据\", True)\n",
    "# jieba.suggest_freq(\"智能终端\", True)\n",
    "# jieba.suggest_freq(\"长达\", True)\n",
    "# jieba.suggest_freq(\"低带宽\", True)\n",
    "# jieba.suggest_freq(\"莫伦科夫\", True)\n",
    "# jieba.suggest_freq(\"乐视网\", True)\n",
    "# jieba.suggest_freq(\"饿了吗\", True)\n",
    "# jieba.suggest_freq(\"于亚辉\", True)\n",
    "# jieba.suggest_freq(\"配送员\", True)\n",
    "# jieba.suggest_freq(\"资金链\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Xinhua(Documents):\n",
    "    \"\"\"\n",
    "    新华网\n",
    "    \"\"\"\n",
    "    __doc_stems_list = []\n",
    "    __doc_file_name_list = []\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        reg_percentage = re.compile(\"[0-9.]+%\")\n",
    "#         path = \"/Users/aron/xinhua/txt_xinhua\"\n",
    "        doc_array=[]\n",
    "        for file_name in os.listdir(path):\n",
    "            word_list = []\n",
    "            if not file_name.endswith(\"txt\"):\n",
    "                continue\n",
    "        #     print(os.path.join(path, file_name))\n",
    "\n",
    "            doc = []\n",
    "            with open(os.path.join(path, file_name), \"r\") as txt:\n",
    "                for i, line in enumerate(txt):\n",
    "                    if i == 0 or line.strip() == \"\" :\n",
    "                        continue\n",
    "                    ll = jieba.cut(line.strip())\n",
    "\n",
    "                    for w in ll:\n",
    "                        if Tokenizer.is_ascii(w) or Tokenizer.if_stopword(w) or Tokenizer.is_number(w) or w ==\" \":\n",
    "                            continue\n",
    "                        m = reg_percentage.match(w)\n",
    "                        if m:\n",
    "                            continue\n",
    "                        if len(w) == 1 and ord(w) < 256:\n",
    "                            continue\n",
    "                        \n",
    "                        if len(w) == 1 and ord(\"Ａ\") <= ord(w) <= ord(\"ｚ\"):\n",
    "                            continue\n",
    "                        \n",
    "                        word_list.append(w)\n",
    "            self.__doc_stems_list.append(word_list)\n",
    "            self.__doc_file_name_list.append(file_name)\n",
    "\n",
    "    def get_doc_name_list(self):\n",
    "        return self.__doc_file_name_list\n",
    "\n",
    "    def get_doc_stems_list(self):\n",
    "        return self.__doc_stems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    # __docs = None\n",
    "    def __init__(self, documents, n_topic_num):\n",
    "        self.__docs = documents\n",
    "        # super(LDA, self).__init__(doc_stems_list))\n",
    "        vectorizer = CountVectorizer(tokenizer = lambda a: a, analyzer = lambda a: a)\n",
    "        # vectorizer.fit(doc for doc in doc_list)\n",
    "        vector_list = vectorizer.fit_transform(self.__docs.get_doc_stems_list())\n",
    "\n",
    "        self.__feat_names = vectorizer.get_feature_names()\n",
    "        # pd_vectorlist = pd.DataFrame(vector_list)#, columns=feat_names)\n",
    "        # pd_vectorlist.to_csv(\"vectorlist.csv\")\n",
    "        self.__n_topic_num = n_topic_num\n",
    "        self.__model = LatentDirichletAllocation(n_components=n_topic_num,\n",
    "                                            doc_topic_prior=0.02,\n",
    "                                            topic_word_prior=0.05,\n",
    "                                            max_iter=50,\n",
    "                                            learning_method='online',\n",
    "                                            learning_offset=50.,\n",
    "                                            random_state=0)\n",
    "        self.__model.fit(vector_list)\n",
    "        self.__doc_topic_dist = self.__model.transform(vector_list)\n",
    "        \n",
    "    def feat_names(self):\n",
    "        return self.__feat_names \n",
    "\n",
    "    def word_distribution(self):\n",
    "        normalize_components = np.array(self.__model.components_ / np.matrix (self.__model.components_.sum(axis=1)).T)\n",
    "        return normalize_components\n",
    "\n",
    "    def word_distribution_csv(self, path = \"nce_words_dist.csv\"):\n",
    "        normalize_components = np.array(self.__model.components_ / np.matrix (self.__model.components_.sum(axis=1)).T)\n",
    "        wordprob = pd.DataFrame(normalize_components, index =[\"#%02d\"%(i) for i in range(self.__n_topic_num)])\n",
    "        feat_name_dict = dict(zip(range(len(self.__feat_names )), self.__feat_names ))\n",
    "        wordprob.rename(columns = feat_name_dict, inplace=True)\n",
    "        w = wordprob.transpose()\n",
    "        w.to_csv(path)\n",
    "        return w\n",
    "\n",
    "    def topic_distribution(self):\n",
    "        return self.__doc_topic_dist\n",
    "\n",
    "    def topic_distribution_csv(self, path=\"nce_topics_dist.csv\"):\n",
    "        pd_doc_topic_dist = pd.DataFrame(self.__doc_topic_dist, index = self.__docs.get_doc_name_list())\n",
    "\n",
    "        topic_name_dict = dict(zip(range(self.__n_topic_num),[\"#%02d\"%(i) for i in range(self.__n_topic_num)]))\n",
    "\n",
    "        pd_doc_topic_dist.rename(columns = topic_name_dict, inplace=True)\n",
    "        # pd_doc_topic_dist.to_csv(\"enpod_topics_data.csv\")\n",
    "        dir_name = os.path.dirname(path)\n",
    "        base_name = os.path.basename(path)\n",
    "\n",
    "        title, ext = os.path.splitext(base_name)\n",
    "        filename_t = title + \"_T\" + ext\n",
    "        pd_doc_topic_dist.transpose().to_csv(os.path.join(dir_name, filename_t))\n",
    "        pd_doc_topic_dist.to_csv(path)\n",
    "        return pd_doc_topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3d/_ctd1d7x0ln5jqs_78y4__6m0000gp/T/jieba.cache\n",
      "Loading model cost 1.140 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# docs = EpDOC( \"./enpod\")\n",
    "# docs = NceDOC(\"../nce/all.txt\")\n",
    "path = \"/Users/aron/xinhua/txt_xinhua\"\n",
    "docs = Xinhua(path)\n",
    "# print(docs.feat_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for doc_name in docs.get_doc_name_list():\n",
    "    # print(doc_name)\n",
    "jieba.load_userdict(\"mydict_cn.txt\")\n",
    "lda = LDA (docs, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25733\n"
     ]
    }
   ],
   "source": [
    "print(len(lda.feat_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics_dist = lda.topic_distribution_csv(path = \"xinhua_topics_dist.csv\")\n",
    "word_dist = lda.word_distribution_csv(path = \"xinhua_word_distribution.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dist.to_csv(\"xinhua_word_distribution.csv\", encoding=\"utf_8_sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
